{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41f1d1e6-de38-41a6-af20-1a60148eccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836f4f8e-9145-4d00-996d-d0e2fce26db5",
   "metadata": {},
   "source": [
    "### Define Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc1b4d2-c7f9-467c-92cf-c7daf66cca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "class nmRosenbrock(nn.Module):\n",
    "    \"\"\"\n",
    "    Penalty loss function for Rosenbrock problem\n",
    "    \"\"\"\n",
    "    def __init__(self, input_keys, steepness, num_blocks, penalty_weight=50, output_key=\"loss\"):\n",
    "        super().__init__()\n",
    "        self.b_key, self.a_key, self.x_key = input_keys\n",
    "        self.output_key = output_key\n",
    "        self.steepness = steepness\n",
    "        self.num_blocks = num_blocks\n",
    "        self.penalty_weight = penalty_weight\n",
    "        self.device = None\n",
    "        # coefs\n",
    "        rng = np.random.RandomState(17)\n",
    "        P = rng.normal(scale=1, size=(3, num_blocks))\n",
    "        q = rng.normal(scale=1, size=(num_blocks))\n",
    "        self.P = torch.from_numpy(P).float()\n",
    "        self.q = torch.from_numpy(q).float()\n",
    "        \n",
    "\n",
    "    def forward(self, input_dict):\n",
    "        \"\"\"\n",
    "        forward pass\n",
    "        \"\"\"\n",
    "        # objective function\n",
    "        obj = self.cal_obj(input_dict)\n",
    "        # constraints violation\n",
    "        viol = self.cal_constr_viol(input_dict)\n",
    "        # penalized loss\n",
    "        loss = obj + self.penalty_weight * viol\n",
    "        input_dict[self.output_key] = torch.mean(loss)\n",
    "        return input_dict\n",
    "\n",
    "    def cal_obj(self, input_dict):\n",
    "        \"\"\"\n",
    "        calculate objective function\n",
    "        \"\"\"\n",
    "        # get values\n",
    "        x, a = input_dict[self.x_key], input_dict[self.a_key]\n",
    "        # x_2i\n",
    "        x1 = x[:, ::2]\n",
    "        # x_2i+1\n",
    "        x2 = x[:, 1::2]\n",
    "        # objective function\n",
    "        f = torch.sum((a - x1) ** 2 + self.steepness * (x2 - x1 ** 2) ** 2, dim=1)\n",
    "        return f\n",
    "\n",
    "    def cal_constr_viol(self, input_dict):\n",
    "        \"\"\"\n",
    "        calculate constraints violation\n",
    "        \"\"\"\n",
    "        # get values\n",
    "        x, b = input_dict[self.x_key], input_dict[self.b_key]\n",
    "        # update device\n",
    "        if self.device is None:\n",
    "            self.device = x.device\n",
    "            self.P = self.P.to(self.device)\n",
    "            self.q = self.q.to(self.device)\n",
    "        # inner constraint violation\n",
    "        lhs_inner = torch.sum(x[:, 1::2], dim=1)\n",
    "        rhs_inner = self.num_blocks * b[:, 0] / 2\n",
    "        inner_violation = torch.relu(rhs_inner - lhs_inner)\n",
    "        # outer constraint violation\n",
    "        lhs_outer = torch.sum(x[:, ::2] ** 2, dim=1)\n",
    "        rhs_outer = self.num_blocks * b[:, 0]\n",
    "        outer_violation = torch.relu(lhs_outer - rhs_outer)\n",
    "        # lear constraint violation\n",
    "        lhs = torch.matmul(x[:, 1::2], self.q)\n",
    "        linear_violation = torch.relu(lhs)\n",
    "        return inner_violation + outer_violation + linear_violation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f873aad0-1ee5-4bac-89d2-4eb22e063418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyomo import environ as pe\n",
    "from src.problem.math_solver import abcParamSolver\n",
    "\n",
    "class msRosenbrock(abcParamSolver):\n",
    "    def __init__(self, steepness, num_blocks, timelimit=None):\n",
    "        super().__init__(timelimit=timelimit)\n",
    "        # create model\n",
    "        m = pe.ConcreteModel()\n",
    "        # parameters\n",
    "        m.b = pe.Param(default=1, mutable=True)\n",
    "        m.a = pe.Param(pe.RangeSet(0, num_blocks-1), default=1, mutable=True)\n",
    "        # variables\n",
    "        m.x = pe.Var(pe.RangeSet(0, num_blocks*2-1), domain=pe.Reals)\n",
    "        for i in range(num_blocks):\n",
    "            # integer variables\n",
    "            m.x[2*i+1].domain = pe.Integers\n",
    "        # objective\n",
    "        obj = sum((m.a[i] - m.x[2*i]) ** 2 + \\\n",
    "                   steepness * (m.x[2*i+1] - m.x[2*i] ** 2) ** 2 for i in range(num_blocks))\n",
    "        m.obj = pe.Objective(sense=pe.minimize, expr=obj)\n",
    "        # constraints\n",
    "        m.cons = pe.ConstraintList()\n",
    "        m.cons.add(sum(m.x[2*i+1] for i in range(num_blocks)) >= num_blocks * m.b / 2)\n",
    "        m.cons.add(sum(m.x[2*i] ** 2 for i in range(num_blocks)) <= num_blocks * m.b)\n",
    "        rng = np.random.RandomState(17)\n",
    "        b = rng.normal(scale=1, size=(3, num_blocks))\n",
    "        q = rng.normal(scale=1, size=(num_blocks))\n",
    "        m.cons.add(sum(b[0,i] * m.x[2*i] for i in range(num_blocks)) == 0)\n",
    "        m.cons.add(sum(b[1,i] * m.x[2*i] for i in range(num_blocks)) == 0)\n",
    "        m.cons.add(sum(b[2,i] * m.x[2*i] for i in range(num_blocks)) == 0)\n",
    "        m.cons.add(sum(q[i] * m.x[2*i+1] for i in range(num_blocks)) <= 0)\n",
    "        # attribute\n",
    "        self.model = m\n",
    "        self.params ={\"b\":m.b, \"a\":m.a}\n",
    "        self.vars = {\"x\":m.x}\n",
    "        self.cons = m.cons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbee0595-6430-4ed2-8988-ccdb83d743ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import null_space\n",
    "\n",
    "class rosenbrockNullSpaceEncoding(nn.Module):\n",
    "    def __init__(self, num_blocks, input_key, output_key):\n",
    "        \"\"\"\n",
    "        encode equality constraints P x = 0 using null space decomposition.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # size\n",
    "        self.num_blocks = num_blocks\n",
    "        # data keys\n",
    "        self.input_key = input_key\n",
    "        self.output_key = output_key\n",
    "        # init encoding\n",
    "        rng = np.random.RandomState(17)\n",
    "        P = rng.normal(scale=1, size=(3, self.num_blocks))\n",
    "        # sepecial solution for equality constraints\n",
    "        x_s, _, _, _ = np.linalg.lstsq(P, np.zeros(3), rcond=None)\n",
    "        # null space for equality constraints\n",
    "        N = null_space(P)\n",
    "        # to pytorch\n",
    "        self.x_s = torch.tensor(x_s, dtype=torch.float32).view(1, -1)\n",
    "        self.N = torch.tensor(N, dtype=torch.float32)\n",
    "        # init device\n",
    "        self.device = None\n",
    "\n",
    "    def forward(self, data):\n",
    "        # get free parameters\n",
    "        z = data[self.input_key]\n",
    "        # batch size\n",
    "        batch_size = z.shape[0]\n",
    "        # device\n",
    "        if z.device != self.device:\n",
    "            self.device = z.device\n",
    "            self.x_s = self.x_s.to(self.device)\n",
    "            self.N = self.N.to(self.device)\n",
    "        # init x\n",
    "        x = torch.zeros((batch_size, self.num_blocks*2)).to(self.device)\n",
    "        # integer part\n",
    "        x[:, 1::2] = z[:,self.num_blocks-3:]\n",
    "        # continous part  to encode\n",
    "        x[:, 0::2] = self.x_s + torch.einsum(\"bj,ij->bi\", z[:,:self.num_blocks-3], self.N)\n",
    "        data[self.output_key] = x\n",
    "        # cut off z\n",
    "        data[self.input_key] = z[:,:self.num_blocks-3]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e876505-51b6-4011-ae8a-b27459a6e971",
   "metadata": {},
   "source": [
    "### Problem Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b78e3dd1-f9be-438f-9b40-6745f4f285af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init\n",
    "steepness = 50    # steepness factor\n",
    "num_blocks = 100  # number of expression blocks\n",
    "num_data = 9100   # number of data\n",
    "test_size = 100   # number of test size\n",
    "val_size = 1000   # number of validation size\n",
    "train_size = num_data - test_size - val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d040f1a-56b0-4c94-ba7a-921c2c4d93c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters as input data\n",
    "b_low, b_high = 1.0, 8.0\n",
    "a_low, a_high = 0.5, 4.5\n",
    "b_train = np.random.uniform(b_low, b_high, (train_size, 1)).astype(np.float32)\n",
    "b_test  = np.random.uniform(b_low, b_high, (test_size, 1)).astype(np.float32)\n",
    "b_dev   = np.random.uniform(b_low, b_high, (val_size, 1)).astype(np.float32)\n",
    "a_train = np.random.uniform(a_low, a_high, (train_size, num_blocks)).astype(np.float32)\n",
    "a_test  = np.random.uniform(a_low, a_high, (test_size, num_blocks)).astype(np.float32)\n",
    "a_dev   = np.random.uniform(a_low, a_high, (val_size, num_blocks)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c3ea095-2bd8-4ecc-ab73-ce42cb66b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nm datasets\n",
    "from neuromancer.dataset import DictDataset\n",
    "data_train = DictDataset({\"b\":b_train, \"a\":a_train}, name=\"train\")\n",
    "data_test = DictDataset({\"b\":b_test, \"a\":a_test}, name=\"test\")\n",
    "data_dev = DictDataset({\"b\":b_dev, \"a\":a_dev}, name=\"dev\")\n",
    "# torch dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "loader_train = DataLoader(data_train, batch_size, num_workers=0, collate_fn=data_train.collate_fn, shuffle=True)\n",
    "loader_test = DataLoader(data_test, batch_size, num_workers=0, collate_fn=data_test.collate_fn, shuffle=False)\n",
    "loader_dev = DataLoader(data_dev, batch_size, num_workers=0, collate_fn=data_dev.collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0471af2-eb01-4b23-8cb0-204c23ac144a",
   "metadata": {},
   "source": [
    "### Exact Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e55e7bf9-8897-49af-9719-fb77df0f349e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = msRosenbrock(steepness, num_blocks, timelimit=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbf896f-b041-4453-85f0-b8fdfdc0423f",
   "metadata": {},
   "source": [
    "### Rounding Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d38a02-c027-48e6-9205-08e3ad682dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b2ee933c-27af-4aa9-9fc2-8ceed279534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 100  # weight of constraint violation penealty\n",
    "hlayers_sol = 5       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 64            # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12b89626-b966-42de-89e4-08689910b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundGumbelModel\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_blocks+1, outsize=2*num_blocks-3, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"b\", \"a\"], [\"z\"], name=\"smap\")\n",
    "# linear constraint encode\n",
    "encoding = rosenbrockNullSpaceEncoding(num_blocks, input_key=\"z\", output_key=\"x\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=3*num_blocks+1, hidden_dims=[hsize]*hlayers_rnd, output_dim=2*num_blocks-3)\n",
    "rnd = roundGumbelModel(layers=layers_rnd, param_keys=[\"b\", \"a\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                       int_ind=model.int_ind, continuous_update=True, equality_encoding=encoding, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = nn.ModuleList([smap, encoding, rnd]).to(\"cuda\")\n",
    "loss_fn = nmRosenbrock([\"b\", \"a\", \"x_rnd\"], steepness, num_blocks, penalty_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53854d39-8dc3-4228-8c15-0c0e894d7595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iters 0, Validation Loss: 25769.48\n",
      "Epoch 0, Iters 125, Training Loss: 12094.68, Validation Loss: 4129.36\n",
      "Epoch 1, Iters 250, Training Loss: 2738.61, Validation Loss: 1599.29\n",
      "Epoch 2, Iters 375, Training Loss: 2032.03, Validation Loss: 1433.68\n",
      "Epoch 3, Iters 500, Training Loss: 1822.36, Validation Loss: 1152.26\n",
      "Epoch 4, Iters 625, Training Loss: 1507.45, Validation Loss: 1156.96\n",
      "Epoch 5, Iters 750, Training Loss: 1431.31, Validation Loss: 1204.62\n",
      "Epoch 6, Iters 875, Training Loss: 1327.99, Validation Loss: 975.45\n",
      "Epoch 7, Iters 1000, Training Loss: 1283.07, Validation Loss: 862.90\n",
      "Epoch 8, Iters 1125, Training Loss: 1128.29, Validation Loss: 931.25\n",
      "Epoch 9, Iters 1250, Training Loss: 1135.00, Validation Loss: 886.01\n",
      "Epoch 10, Iters 1375, Training Loss: 1136.96, Validation Loss: 826.25\n",
      "Epoch 11, Iters 1500, Training Loss: 1058.53, Validation Loss: 777.54\n",
      "Epoch 12, Iters 1625, Training Loss: 956.90, Validation Loss: 766.02\n",
      "Epoch 13, Iters 1750, Training Loss: 957.97, Validation Loss: 835.03\n",
      "Epoch 14, Iters 1875, Training Loss: 892.40, Validation Loss: 684.05\n",
      "Epoch 15, Iters 2000, Training Loss: 830.43, Validation Loss: 628.15\n",
      "Epoch 16, Iters 2125, Training Loss: 810.49, Validation Loss: 665.10\n",
      "Epoch 17, Iters 2250, Training Loss: 779.02, Validation Loss: 703.76\n",
      "Epoch 18, Iters 2375, Training Loss: 774.83, Validation Loss: 570.38\n",
      "Epoch 19, Iters 2500, Training Loss: 744.21, Validation Loss: 566.26\n",
      "Epoch 20, Iters 2625, Training Loss: 754.62, Validation Loss: 532.84\n",
      "Epoch 21, Iters 2750, Training Loss: 709.01, Validation Loss: 588.00\n",
      "Epoch 22, Iters 2875, Training Loss: 746.96, Validation Loss: 576.60\n",
      "Epoch 23, Iters 3000, Training Loss: 719.23, Validation Loss: 531.90\n",
      "Epoch 24, Iters 3125, Training Loss: 716.15, Validation Loss: 544.03\n",
      "Epoch 25, Iters 3250, Training Loss: 732.10, Validation Loss: 712.62\n",
      "Epoch 26, Iters 3375, Training Loss: 705.65, Validation Loss: 600.59\n",
      "Epoch 27, Iters 3500, Training Loss: 724.00, Validation Loss: 580.65\n",
      "Epoch 28, Iters 3625, Training Loss: 730.25, Validation Loss: 516.42\n",
      "Epoch 29, Iters 3750, Training Loss: 736.66, Validation Loss: 579.81\n",
      "Epoch 30, Iters 3875, Training Loss: 707.22, Validation Loss: 542.54\n",
      "Epoch 31, Iters 4000, Training Loss: 662.12, Validation Loss: 542.60\n",
      "Epoch 32, Iters 4125, Training Loss: 723.85, Validation Loss: 805.23\n",
      "Epoch 33, Iters 4250, Training Loss: 716.29, Validation Loss: 626.39\n",
      "Epoch 34, Iters 4375, Training Loss: 696.61, Validation Loss: 558.86\n",
      "Epoch 35, Iters 4500, Training Loss: 726.94, Validation Loss: 554.93\n",
      "Epoch 36, Iters 4625, Training Loss: 683.76, Validation Loss: 558.45\n",
      "Epoch 37, Iters 4750, Training Loss: 669.70, Validation Loss: 598.29\n",
      "Epoch 38, Iters 4875, Training Loss: 724.87, Validation Loss: 546.92\n",
      "Epoch 39, Iters 5000, Training Loss: 711.21, Validation Loss: 581.96\n",
      "Epoch 40, Iters 5125, Training Loss: 694.10, Validation Loss: 572.07\n",
      "Epoch 41, Iters 5250, Training Loss: 701.53, Validation Loss: 692.36\n",
      "Epoch 42, Iters 5375, Training Loss: 715.07, Validation Loss: 539.18\n",
      "Epoch 43, Iters 5500, Training Loss: 713.23, Validation Loss: 628.44\n",
      "Epoch 44, Iters 5625, Training Loss: 713.47, Validation Loss: 690.40\n",
      "Epoch 45, Iters 5750, Training Loss: 738.38, Validation Loss: 658.24\n",
      "Epoch 46, Iters 5875, Training Loss: 799.17, Validation Loss: 581.89\n",
      "Epoch 47, Iters 6000, Training Loss: 745.71, Validation Loss: 686.22\n",
      "Epoch 48, Iters 6125, Training Loss: 767.23, Validation Loss: 2164.64\n",
      "Early stopping at iters 6125\n",
      "Best model loaded.\n",
      "Training complete.\n",
      "The training time is 102.98 sec.\n"
     ]
    }
   ],
   "source": [
    "from src.problem.neuromancer.trainer import trainer\n",
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.AdamW(components.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "my_trainer = trainer(components, loss_fn, optimizer, epochs=epochs, patience=patience, warmup=warmup, device=\"cuda\")\n",
    "# training for the rounding problem\n",
    "my_trainer.train(loader_train, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5740c26-8e56-4273-b194-c137dc105b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 51.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Obj Val  Mean Violation  Max Violation  Num Violations  Elapsed Time\n",
      "count  100.000000           100.0          100.0           100.0    100.000000\n",
      "mean   497.850032             0.0            0.0             0.0      0.004028\n",
      "std     86.855250             0.0            0.0             0.0      0.001570\n",
      "min    312.995691             0.0            0.0             0.0      0.002000\n",
      "25%    429.101735             0.0            0.0             0.0      0.003003\n",
      "50%    488.077553             0.0            0.0             0.0      0.003514\n",
      "75%    561.077152             0.0            0.0             0.0      0.004394\n",
      "max    689.484268             0.0            0.0             0.0      0.010808\n",
      "Number of infeasible solution: 0\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, mean_viols, max_viols, num_viols, elapseds = [], [], [], [], [], [], []\n",
    "for b, a in tqdm(list(zip(b_test, a_test))):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"b\": torch.tensor(np.array([b]), dtype=torch.float32).to(\"cuda\"), \n",
    "                  \"a\": torch.tensor(np.array([a]), dtype=torch.float32).to(\"cuda\"),\n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    components.eval()\n",
    "    tick = time.time()\n",
    "    with torch.no_grad():\n",
    "        for comp in components:\n",
    "            datapoints.update(comp(datapoints))\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"b\":b, \"a\":a})\n",
    "    # assign vars\n",
    "    x = datapoints[\"x_rnd\"]\n",
    "    for i in range(2*num_blocks):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(b)+list(a))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    viol = model.cal_violation()\n",
    "    mean_viols.append(np.mean(viol))\n",
    "    max_viols.append(np.max(viol))\n",
    "    num_viols.append(np.sum(viol > 1e-6))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\": params,\n",
    "                    \"Sol\": sols,\n",
    "                    \"Obj Val\": objvals,\n",
    "                    \"Mean Violation\": mean_viols,\n",
    "                    \"Max Violation\": max_viols,\n",
    "                    \"Num Violations\": num_viols,\n",
    "                    \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Num Violations\"] > 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ef6869-02d0-48af-8deb-c8f088271332",
   "metadata": {},
   "source": [
    "### Learnable Threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "631e7fa1-3857-463b-babd-ae78f4e3dd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "286bb184-586d-4cde-80fe-311d5c426abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "penalty_weight = 100  # weight of constraint violation penealty\n",
    "hlayers_sol = 5       # number of hidden layers for solution mapping\n",
    "hlayers_rnd = 4       # number of hidden layers for solution mapping\n",
    "hsize = 64            # width of hidden layers for solution mapping\n",
    "lr = 1e-3             # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36dfc237-c232-404c-ba41-877099c0008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set problem\n",
    "import neuromancer as nm\n",
    "from src.func.layer import netFC\n",
    "from src.func import roundThresholdModel\n",
    "# build neural architecture for the solution map\n",
    "func = nm.modules.blocks.MLP(insize=num_blocks+1, outsize=2*num_blocks-3, bias=True,\n",
    "                             linear_map=nm.slim.maps[\"linear\"],\n",
    "                             nonlin=nn.ReLU, hsizes=[hsize]*hlayers_sol)\n",
    "smap = nm.system.Node(func, [\"b\", \"a\"], [\"z\"], name=\"smap\")\n",
    "# linear constraint encode\n",
    "encoding = rosenbrockNullSpaceEncoding(num_blocks, input_key=\"z\", output_key=\"x\")\n",
    "# define rounding model\n",
    "layers_rnd = netFC(input_dim=3*num_blocks+1, hidden_dims=[hsize]*hlayers_rnd, output_dim=2*num_blocks-3)\n",
    "rnd = roundThresholdModel(layers=layers_rnd, param_keys=[\"b\", \"a\"], var_keys=[\"x\"],  output_keys=[\"x_rnd\"], \n",
    "                          int_ind=model.int_ind, continuous_update=True, equality_encoding=encoding, name=\"round\")\n",
    "# build neuromancer problem for rounding\n",
    "components = nn.ModuleList([smap, encoding, rnd]).to(\"cuda\")\n",
    "loss_fn = nmRosenbrock([\"b\", \"a\", \"x_rnd\"], steepness, num_blocks, penalty_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c38c697f-2297-40fb-b1dc-521561c9e462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Iters 0, Validation Loss: 22756.81\n",
      "Epoch 0, Iters 125, Training Loss: 10810.08, Validation Loss: 2614.14\n",
      "Epoch 1, Iters 250, Training Loss: 1935.77, Validation Loss: 1301.65\n",
      "Epoch 2, Iters 375, Training Loss: 1556.54, Validation Loss: 1053.93\n",
      "Epoch 3, Iters 500, Training Loss: 1339.13, Validation Loss: 1026.92\n",
      "Epoch 4, Iters 625, Training Loss: 1292.43, Validation Loss: 927.83\n",
      "Epoch 5, Iters 750, Training Loss: 1224.03, Validation Loss: 1657.92\n",
      "Epoch 6, Iters 875, Training Loss: 1224.14, Validation Loss: 1211.57\n",
      "Epoch 7, Iters 1000, Training Loss: 999.39, Validation Loss: 837.11\n",
      "Epoch 8, Iters 1125, Training Loss: 928.84, Validation Loss: 745.00\n",
      "Epoch 9, Iters 1250, Training Loss: 873.65, Validation Loss: 697.63\n",
      "Epoch 10, Iters 1375, Training Loss: 822.52, Validation Loss: 746.11\n",
      "Epoch 11, Iters 1500, Training Loss: 801.90, Validation Loss: 702.92\n",
      "Epoch 12, Iters 1625, Training Loss: 786.91, Validation Loss: 656.25\n",
      "Epoch 13, Iters 1750, Training Loss: 765.94, Validation Loss: 634.51\n",
      "Epoch 14, Iters 1875, Training Loss: 747.92, Validation Loss: 698.09\n",
      "Epoch 15, Iters 2000, Training Loss: 764.88, Validation Loss: 648.14\n",
      "Epoch 16, Iters 2125, Training Loss: 723.42, Validation Loss: 657.89\n",
      "Epoch 17, Iters 2250, Training Loss: 727.67, Validation Loss: 628.52\n",
      "Epoch 18, Iters 2375, Training Loss: 712.41, Validation Loss: 598.45\n",
      "Epoch 19, Iters 2500, Training Loss: 695.70, Validation Loss: 620.73\n",
      "Epoch 20, Iters 2625, Training Loss: 709.12, Validation Loss: 672.65\n",
      "Epoch 21, Iters 2750, Training Loss: 698.46, Validation Loss: 607.49\n",
      "Epoch 22, Iters 2875, Training Loss: 687.12, Validation Loss: 593.97\n",
      "Epoch 23, Iters 3000, Training Loss: 669.25, Validation Loss: 668.04\n",
      "Epoch 24, Iters 3125, Training Loss: 684.14, Validation Loss: 624.09\n",
      "Epoch 25, Iters 3250, Training Loss: 701.17, Validation Loss: 664.91\n",
      "Epoch 26, Iters 3375, Training Loss: 673.26, Validation Loss: 591.48\n",
      "Epoch 27, Iters 3500, Training Loss: 668.51, Validation Loss: 601.53\n",
      "Epoch 28, Iters 3625, Training Loss: 681.98, Validation Loss: 670.12\n",
      "Epoch 29, Iters 3750, Training Loss: 673.42, Validation Loss: 797.23\n",
      "Epoch 30, Iters 3875, Training Loss: 698.14, Validation Loss: 569.25\n",
      "Epoch 31, Iters 4000, Training Loss: 688.57, Validation Loss: 624.15\n",
      "Epoch 32, Iters 4125, Training Loss: 723.80, Validation Loss: 622.60\n",
      "Epoch 33, Iters 4250, Training Loss: 747.02, Validation Loss: 740.96\n",
      "Epoch 34, Iters 4375, Training Loss: 725.56, Validation Loss: 617.40\n",
      "Epoch 35, Iters 4500, Training Loss: 750.41, Validation Loss: 650.02\n",
      "Epoch 36, Iters 4625, Training Loss: 800.36, Validation Loss: 971.12\n",
      "Epoch 37, Iters 4750, Training Loss: 738.35, Validation Loss: 597.51\n",
      "Epoch 38, Iters 4875, Training Loss: 793.00, Validation Loss: 726.17\n",
      "Epoch 39, Iters 5000, Training Loss: 725.99, Validation Loss: 727.76\n",
      "Epoch 40, Iters 5125, Training Loss: 760.77, Validation Loss: 862.70\n",
      "Epoch 41, Iters 5250, Training Loss: 815.62, Validation Loss: 839.93\n",
      "Epoch 42, Iters 5375, Training Loss: 710.63, Validation Loss: 604.61\n",
      "Epoch 43, Iters 5500, Training Loss: 773.65, Validation Loss: 728.76\n",
      "Epoch 44, Iters 5625, Training Loss: 768.07, Validation Loss: 759.80\n",
      "Epoch 45, Iters 5750, Training Loss: 746.64, Validation Loss: 694.82\n",
      "Epoch 46, Iters 5875, Training Loss: 737.34, Validation Loss: 706.62\n",
      "Epoch 47, Iters 6000, Training Loss: 673.47, Validation Loss: 566.12\n",
      "Epoch 48, Iters 6125, Training Loss: 748.01, Validation Loss: 660.38\n",
      "Epoch 49, Iters 6250, Training Loss: 731.64, Validation Loss: 654.56\n",
      "Epoch 50, Iters 6375, Training Loss: 780.46, Validation Loss: 687.90\n",
      "Epoch 51, Iters 6500, Training Loss: 694.64, Validation Loss: 651.73\n",
      "Epoch 52, Iters 6625, Training Loss: 768.98, Validation Loss: 648.66\n",
      "Epoch 53, Iters 6750, Training Loss: 742.14, Validation Loss: 706.16\n",
      "Epoch 54, Iters 6875, Training Loss: 729.61, Validation Loss: 770.41\n",
      "Epoch 55, Iters 7000, Training Loss: 712.68, Validation Loss: 705.30\n",
      "Epoch 56, Iters 7125, Training Loss: 700.55, Validation Loss: 585.06\n",
      "Epoch 57, Iters 7250, Training Loss: 700.91, Validation Loss: 722.63\n",
      "Epoch 58, Iters 7375, Training Loss: 750.20, Validation Loss: 652.58\n",
      "Epoch 59, Iters 7500, Training Loss: 712.52, Validation Loss: 704.30\n",
      "Epoch 60, Iters 7625, Training Loss: 709.47, Validation Loss: 633.09\n",
      "Epoch 61, Iters 7750, Training Loss: 709.58, Validation Loss: 670.81\n",
      "Epoch 62, Iters 7875, Training Loss: 722.45, Validation Loss: 719.10\n",
      "Epoch 63, Iters 8000, Training Loss: 691.53, Validation Loss: 711.06\n",
      "Epoch 64, Iters 8125, Training Loss: 733.08, Validation Loss: 681.04\n",
      "Epoch 65, Iters 8250, Training Loss: 720.59, Validation Loss: 889.04\n",
      "Epoch 66, Iters 8375, Training Loss: 697.92, Validation Loss: 612.51\n",
      "Epoch 67, Iters 8500, Training Loss: 670.69, Validation Loss: 684.37\n",
      "Early stopping at iters 8500\n",
      "Best model loaded.\n",
      "Training complete.\n",
      "The training time is 133.24 sec.\n"
     ]
    }
   ],
   "source": [
    "from src.problem.neuromancer.trainer import trainer\n",
    "# training\n",
    "epochs = 200                    # number of training epochs\n",
    "warmup = 20                     # number of epochs to wait before enacting early stopping policy\n",
    "patience = 20                   # number of epochs with no improvement in eval metric to allow before early stopping\n",
    "optimizer = torch.optim.AdamW(components.parameters(), lr=lr)\n",
    "# create a trainer for the problem\n",
    "my_trainer = trainer(components, loss_fn, optimizer, epochs=epochs, patience=patience, warmup=warmup, device=\"cuda\")\n",
    "# training for the rounding problem\n",
    "my_trainer.train(loader_train, loader_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d16528d-8174-45a6-bce1-c6dd507ceb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 57.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Obj Val  Mean Violation  Max Violation  Num Violations  Elapsed Time\n",
      "count  100.000000           100.0          100.0           100.0    100.000000\n",
      "mean   561.447941             0.0            0.0             0.0      0.003595\n",
      "std    115.675314             0.0            0.0             0.0      0.000971\n",
      "min    334.502777             0.0            0.0             0.0      0.001998\n",
      "25%    498.454217             0.0            0.0             0.0      0.003002\n",
      "50%    553.078944             0.0            0.0             0.0      0.003436\n",
      "75%    616.772741             0.0            0.0             0.0      0.004018\n",
      "max    971.378564             0.0            0.0             0.0      0.008277\n",
      "Number of infeasible solution: 0\n"
     ]
    }
   ],
   "source": [
    "params, sols, objvals, mean_viols, max_viols, num_viols, elapseds = [], [], [], [], [], [], []\n",
    "for b, a in tqdm(list(zip(b_test, a_test))):\n",
    "    # data point as tensor\n",
    "    datapoints = {\"b\": torch.tensor(np.array([b]), dtype=torch.float32).to(\"cuda\"), \n",
    "                  \"a\": torch.tensor(np.array([a]), dtype=torch.float32).to(\"cuda\"),\n",
    "                  \"name\": \"test\"}\n",
    "    # infer\n",
    "    components.eval()\n",
    "    tick = time.time()\n",
    "    with torch.no_grad():\n",
    "        for comp in components:\n",
    "            datapoints.update(comp(datapoints))\n",
    "    tock = time.time()\n",
    "    # assign params\n",
    "    model.set_param_val({\"b\":b, \"a\":a})\n",
    "    # assign vars\n",
    "    x = datapoints[\"x_rnd\"]\n",
    "    for i in range(2*num_blocks):\n",
    "        model.vars[\"x\"][i].value = x[0,i].item()\n",
    "    # get solutions\n",
    "    xval, objval = model.get_val()    \n",
    "    params.append(list(b)+list(a))\n",
    "    sols.append(list(list(xval.values())[0].values()))\n",
    "    objvals.append(objval)\n",
    "    viol = model.cal_violation()\n",
    "    mean_viols.append(np.mean(viol))\n",
    "    max_viols.append(np.max(viol))\n",
    "    num_viols.append(np.sum(viol > 1e-6))\n",
    "    elapseds.append(tock - tick)\n",
    "df = pd.DataFrame({\"Param\": params,\n",
    "                    \"Sol\": sols,\n",
    "                    \"Obj Val\": objvals,\n",
    "                    \"Mean Violation\": mean_viols,\n",
    "                    \"Max Violation\": max_viols,\n",
    "                    \"Num Violations\": num_viols,\n",
    "                    \"Elapsed Time\": elapseds})\n",
    "time.sleep(1)\n",
    "print(df.describe())\n",
    "print(\"Number of infeasible solution: {}\".format(np.sum(df[\"Num Violations\"] > 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8cea1f-9b7d-42ac-b89f-ed57ad675de5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
